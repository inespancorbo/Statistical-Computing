{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 {-}\n",
    "\n",
    "**Consider the Chutes and Ladders game of our homework 3, problem 2c, in which a player wraps around to the start of the board once the $100$th square is passed. Let $X(t)$ be the position of the player at round $t$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a {-}\n",
    "\n",
    "**Compute the entropy of $X(t)$ for $t = 1,2,...,200$. Plot the entropy with $t$ and comment on the pattern.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We let $X(t)$ represent the square the player occupies at time $t$. We have that $t \\in \\{0, 1, 2, 3, \\ldots, 200\\}$ and that the state space, $S$, is $S=\\{0, 1, 2, 3, 5, \\dots, 100\\}$. Let us assume $X_0 = 0$, i.e, the player starts with her/his piece off the board.\n",
    "\n",
    "Then,\n",
    "\n",
    "$$P(X(t)=i)=\\sum_{t_1=0}^{100}\\sum_{t_1=0}^{100}\\ldots\\sum_{t_{t-1}=0}^{100}P(X(0)=0, X(1)=t_1, \\ldots, X(t-1)=t_{t-1}, X(t)=i)$$\n",
    "\n",
    "which by the Markov property is,\n",
    "\n",
    "$$P(X(t)=i)=\\sum_{t_1=0}^{100}\\sum_{t_1=0}^{100}\\ldots\\sum_{t_{t-1}=0}^{100}P(X(t)=i|X(t-1)=t_{t-1})\\ldots P(X(1)=t_1|X(0)=0)P(X(0))$$\n",
    "\n",
    "And as already seen before in class,\n",
    "\n",
    "$$P(X(t)=i)=\\sum_{l}P(X(t)=i|X(0)=l)P(X(0)=l)=P(X(t)=i|X(0)=0)\\cdot 1 =P^t_{(0, i)}$$\n",
    "\n",
    "and so $P(X(t))=P^t_{(0, \\cdot)}$\n",
    "\n",
    "for $t = 1, 2, 3, . . . , 200$, where $P^t$ means matrix multiplying $P$ $t$ times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropy for $X(t)$ for $t=1,2,\\ldots, 200$ is $H(X(t))=-\\sum_{i=0}^{100}P(X(t)=i)\\log(P(X(t)=i))=-P^t_{(0,\\cdot)}\\cdot \\log P^t_{(0,\\cdot)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use the functions defined in HW 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_P_v2(chutes_and_ladder_locations):\n",
    "    \"\"\"Builds the transition probability matrix for the chutes and ladders game\n",
    "    \n",
    "    Args:\n",
    "        chutes_and_ladder_locations: Dataframe containing the info from \n",
    "        the chutes_and_ladder_locations.csv\n",
    "    \n",
    "    Returns:\n",
    "        A numpy array of shape (101, 101)\n",
    "    \n",
    "    \"\"\"\n",
    "    # helper function\n",
    "    def shift(array):\n",
    "        return(np.concatenate((np.array([0]), array[:-1])))\n",
    "    \n",
    "    # creating the transition probability \n",
    "    # matrix, P, assuming no chutes and ladders\n",
    "    pdf = np.zeros(101)\n",
    "    pdf[1:7] = 1/6\n",
    "    P = pdf.copy()\n",
    "    \n",
    "    for _ in range(100):\n",
    "        pdf = shift(pdf)\n",
    "        P = np.concatenate((P, pdf))\n",
    "  \n",
    "    P = P.reshape(101, 101)\n",
    "    \n",
    "    # fixing P for last squares in game\n",
    "    for i in [95, 96, 97, 98, 99, 100]:\n",
    "        for j in range(1, i+6-99):\n",
    "            P[i, j] = 1/6\n",
    "    \n",
    "    # fixing P for chutes and ladders\n",
    "    for i, j in zip(chutes_and_ladder_locations.start, chutes_and_ladder_locations.end):\n",
    "        P[:,j] += P[:,i]\n",
    "        P[:,i] = np.zeros(101)\n",
    "        \n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "chutes_and_ladder_locations = pd.read_csv('../HW2/chutes_and_ladder_locations.csv')\n",
    "P = build_P_v2(chutes_and_ladder_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(t):\n",
    "    Pt = np.linalg.matrix_power(P, t)\n",
    "    # we assume log0*0=0, exclude 0s to avoid log error\n",
    "    ind = np.nonzero(Pt[0,:])[0]\n",
    "    return - Pt[0,ind].dot(np.log2(Pt[0,ind]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies = map(entropy, range(0,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(0,200), list(entropies), c=\"green\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As time increases, entropy increases. Since we can think of entropy as quantifying how much we will be surprised by the outcome of the random variable $X(t)$ on average, the plot is telling us that as the number of rounds increases the greater our surprise will be, on average, by where we land on the board. This makes sense: On the first round, it is pretty clear where we might land on the board (we have $6$ options), but as time goes by (more rounds pass), it is not clear where our piece will be on the board and hence, what options we will have -- hence, the greater entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b {-}\n",
    "\n",
    "**Let $\\tau = 12345678901234567890$. Compute the relative mutual information of $X (\\tau+t)$ with $X (\\tau)$ for $t = 0, 5, 10, . . . , 50$. Comment on the pattern.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mutual information $I(X(\\tau+t);X(\\tau))$ was defined as \n",
    "\n",
    "$$H(X(\\tau+t))-H(X(\\tau+t)|X(\\tau))$$\n",
    "\n",
    "and so the relative mutual information is \n",
    "\n",
    "$$\\frac{H(X(\\tau+t))-H(X(\\tau+t)|X(\\tau))}{H(X(\\tau+t))}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the conditional entropy $H(X(\\tau+t)|X(\\tau))$ is defined as \n",
    "\n",
    "$$H(X(\\tau+t)|X(\\tau))$$\n",
    "\n",
    "$$=-\\sum_{i=0}^{100}\\sum_{j=0}^{100}P(X(\\tau+t)=i,X(\\tau)=j)\\log P(X(\\tau+t)=i|X(\\tau)=j)$$\n",
    "\n",
    "$$=-\\sum_{i=0}^{100}\\sum_{j=0}^{100}P(X(\\tau+t)=i|X(\\tau)=j)P(X(\\tau)=j)\\log P(X(\\tau+t)=i|X(\\tau)=j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two things to note:\n",
    "\n",
    "1. $P(X(\\tau+t)=i|X(\\tau)=j) = P(X(t)=i|X(0)=j)=(P^t)_{(0,i)}$ by definition of transition probability matrix\n",
    "2. $P(X(\\tau)=j)=P^{\\tau}_{(0,j)}$ as seen above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so,\n",
    "\n",
    "$$=-\\sum_{i=0}^{100}\\sum_{j=0}^{100}(P^t)_{(0,i)}P^{\\tau}_{(0,j)}\\log (P^t)_{(0,i)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$=-\\sum_{j=0}^{100}P^{\\tau}_{(0,j)}[(P^t)_{(0,\\cdot)}\\cdot\\log (P^t)_{(0,\\cdot)}]$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximating $P^{\\tau}_{(0,\\cdot)}$ with the stationary distribution $\\pi$ we have:\n",
    "\n",
    "$$=-\\sum_{j=0}^{100}\\pi_j[(P^t)_{(0,\\cdot)}\\cdot\\log (P^t)_{(0,\\cdot)}]$$\n",
    "\n",
    "This is what is implemented in the conditional_entropy(t) function below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_entropy(t):\n",
    "    Pt = np.linalg.matrix_power(P, t)\n",
    "    # we assume log0*0=0, exclude 0s to avoid log error\n",
    "    ind = np.nonzero(Pt[0,:])[0]\n",
    "    return - np.sum(Ptau)*Pt[0,ind].dot(np.log2(Pt[0,ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the stationary distribution $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues = np.linalg.eig(P.T)[0]\n",
    "eigenvectors = np.linalg.eig(P.T)[1]\n",
    "# Abs gives you mod in python for complex numbers too\n",
    "i = np.argmax(abs(eigenvalues))\n",
    "eigenvector_1 = eigenvectors[:,i]\n",
    "eigenvector_1 = eigenvector_1.real\n",
    "# probability vector\n",
    "eigenvector_1 = abs(eigenvector_1/sum(eigenvector_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tau = 12345678901234567890\n",
    "# Ptau = np.linalg.matrix_power(P, tau)[0,:]\n",
    "Ptau = eigenvector_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get the relative mutual information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the entropy of the stationary distribution\n",
    "ind = np.nonzero(Ptau)[0]\n",
    "entropy_stationary_dist = - Ptau[ind].dot(np.log2(Ptau[ind])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_mutual_information(t):\n",
    "    return(1 - conditional_entropy(t)/entropy_stationary_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "relative_mutual_informations = list(map(relative_mutual_information, range(0, 55, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAdvklEQVR4nO3de5QdZZ3u8e+TTkIIBMKluZh7SCAQKgoTEMY7DhpAxeMN1JGL42RQmMHluAZ1yXGU4zocGTw6ypgVEEEUUQSHyEQQuekoHEPklgQZehIgTRASud8J/M4fbzVsmu7dlaSra+9dz2etWnvXZdf+FZd+dtVb9b6KCMzMrL5GVV2AmZlVy0FgZlZzDgIzs5pzEJiZ1ZyDwMys5kZXXcCm2nnnnWP69OlVl2Fm1laWL1++ISK6B1rXdkEwffp0brrppqrLMDNrK5LuGWydLw2ZmdWcg8DMrOYcBGZmNecgMDOrOQeBmVnNlRYEks6V9KCkFYOsl6R/ldQj6TZJ+5dVi5mZDa7MM4LzgAVN1h8GzM6nhcB3SqzFzMwGUVoQRMSvgYeabHIk8P1IbgQmStq9rHpYsQJOOQUee6y0rzAza0dVthFMAtY2zPfmy15F0kJJN0m6af369Zv3bWvWwNe+lgLBzMxeUmUQaIBlA46SExGLI2J+RMzv7h7wCemhZVl6vf32zfu8mVmHqjIIeoEpDfOTgXWlfdu0aTBhgoPAzKyfKoNgCXBMfvfQQcCjEXF/ad8mwb77OgjMzPoprdM5ST8C3grsLKkX+BIwBiAiFgFLgcOBHuAp4PiyanlJlsHFF0NECgYzMysvCCLiw0OsD+DEsr5/QFkGixfDunUwacB2aTOz2qnXk8VuMDYzexUHgZlZzdUrCHbcMV0SchCYmb2kXkEA6azAQWBm9pJ6BsGqVfD881VXYmbWEuoZBM89B3fdVXUlZmYtoZ5BAL48ZGaWq18Q7L03dHU5CMzMcvULgq22gj33dBCYmeXqFwTgO4fMzBrUNwjWrIHHH6+6EjOzytU3CABWrqy2DjOzFlDPIJg3L7368pCZWU2DYNo02HZbB4GZGXUNglGjPEiNmVmunkEAqZ3gttvSIDVmZjVW7yB46CG4v7zRMc3M2kG9gwB8ecjMas9B4CAws5qrbxDstBPsvruDwMxqr75BAO5qwswMB0EapGbjxqorMTOrjIPg2Wehp6fqSszMKlPvIHBXE2ZmNQ8CD1JjZlbzIBg3DmbPdhCYWa2NHmoDSd3A3wLTG7ePiI+XV9YIyjJYvrzqKszMKjNkEACXAb8BfgW8UG45FcgyuPhieOKJ1COpmVnNFAmC8RFxSumVVKVxkJrXv77aWszMKlCkjeBySYeXXklV3NWEmdVckSA4mRQGz0h6PJ8eK7uwETNjBmyzjYPAzGpryCCIiAkRMSoixuXvJ0TEdkV2LmmBpDsl9Uj63ADrt5f0c0m3Slop6fjNOYgtMmoUzJ3rIDCz2irSRoCk9wBvzmevi4jLC3ymCzgLOBToBZZJWhIRqxo2OxFYFRHvzu9OulPSDyPiuU06ii2VZXDZZWmQGmlEv9rMrGpDnhFIOp10eWhVPp2cLxvKgUBPRKzO/7BfBBzZb5sAJkgSsC3wEDDyHf9kGWzYAA88MOJfbWZWtSJnBIcDr4uIFwEknQ/cDLzqUk8/k4C1DfO9QP/bcr4NLAHWAROAo/q+p5GkhcBCgKlTpxYoeRM1Nhjvttvw79/MrIUVfbJ4YsP77Qt+ZqBrLP0HCH4ncAvwGuB1wLclvar9ISIWR8T8iJjf3d1d8Os3ge8cMrMaK3JG8L+BmyVdS/rj/mbg8wU+1wtMaZifTPrl3+h44PSICKBH0hpgDvD7AvsfPt3d6UzAQWBmNTRkEETEjyRdBxxACoJTIuJPBfa9DJgtaQZwH3A08JF+29wLvB34jaRdgb2A1cXLH0ZZBrfdVslXm5lVadBLQ5Lm5K/7A7uTfuGvBV6TL2sqIjYCJwFXAncAP4mIlZJOkHRCvtlpwF9Kuh24mhQyG7bkgDZb3yA1L3ReLxpmZs00OyP4DKmB9swB1gVwyFA7j4ilwNJ+yxY1vF8HvKNQpWXLMnjmmTRIzV57VV2NmdmIGTQIImJh/vawiHimcZ2kcaVWVYXGBmMHgZnVSJG7hn5XcFl722ef9JSxG4zNrGYGPSOQtBvpWYCtJe3Hy7eDbgeMH4HaRtbWW8OsWQ4CM6udZm0E7wSOI932+fWG5Y8DXyixpupkGdx6a9VVmJmNqGZtBOcD50t6f0RcMoI1VSfL4NJL4cknU4+kZmY1UOQ5gkskHQHMBcY1LP9KmYVVIstSx3OrVsEBB1RdjZnZiCjS6dwi4Cjg70ntBB8EppVcVzXmzUuvbicwsxopctfQX0bEMcDDEfFl4GBe2XVE55g5E8aPdxCYWa0UCYKn89enJL0GeB6YUV5JFfIgNWZWQ0XHLJ4InAH8AbibNLZAZ3KfQ2ZWM0WGqjwtIh7J7xyaBsyJiFPLL60iWQbr13uQGjOrjSHvGsqHnDwCmN63vSQi4uvNPte2Grua2HXXamsxMxsBRS4N/Zz0YNlOpFHE+qbO5EFqzKxmigxMMzki5pVeSavYZZc0OQjMrCaKnBH8QlJrdBU9UrLMQWBmtVEkCG4EfibpaUmPSXpc0mNlF1apLIOVKz1IjZnVQpEgOJP0ENn4iNguIiZExKsGmO8oWQZPPw2rqxk108xsJBUJgruAFfkA8/XgBmMzq5EijcX3A9dJ+gXwbN/Cjr19FNLTxVIKgve9r+pqzMxKVSQI1uTT2HzqfOPHe5AaM6uNpkGQP0w2OyL+eoTqaR3uasLMaqJpG0FEvAB0S6rHmUCjLIOeHnjqqaorMTMrVZFLQ3cDv5W0BHiyb2FHtxHAKwepmT+/6mrMzEpT5K6hdcDl+bad38VEH985ZGY1UWSoyi8DSJqQZuOJ0qtqBXvsAVtv7SAws45XZKjKfSXdDKwAVkpaLmlu+aVVrKsL9tnHQWBmHa/IpaHFwGciYlpETAP+ETi73LJahPscMrMaKBIE20TEtX0zEXEdsE1pFbWSLEsD1KxfX3UlZmalKRIEqyWdKml6Pn2R9IBZ53ODsZnVQJEg+DjQDVyaTzsDx5dZVMuYlw/D4CAwsw42aBBIuiB/e0xE/ENE7J9Pn46Ih4vsXNICSXdK6pH0uUG2eaukWyStlHT9ZhxDeXbdFbq7HQRm1tGa3T76F5KmAR+X9H1AjSsj4qFmO867pzgLOBToBZZJWhIRqxq2mQj8G7AgIu6VtMvmHUaJ3NWEmXW4ZkGwCLgCmAks55VBEPnyZg4EeiJiNYCki4AjgVUN23wEuDQi7gWIiAc3qfqRkGVw9tnw4oswqsiVNDOz9jLoX7aI+NeI2Bs4NyJmRsSMhmmoEACYBKxtmO/NlzXaE9hB0nX58wnHDLQjSQsl3STppvUjfQdPlqX+hjxIjZl1qCF/4kbEJyV1SXqNpKl9U4F9a4Bl/Qe3GQ38BXAE8E7gVEl7DlDD4oiYHxHzu7u7C3z1MPKdQ2bW4Yo8WXwS8ABwFfAf+XR5gX33AlMa5ieT+i3qv80VEfFkRGwAfg28tsC+R07jIDVmZh2oSO+jnwb2iog/b+K+lwGzJc0A7gOOJrUJNLoM+Lak0aRBb14P/N9N/J5ybbMNzJzpIDCzjlUkCNYCj27qjiNiY342cSXQRWprWCnphHz9ooi4Q9IVwG3Ai8A5EbFiU7+rdO5qwsw6WJEgWE0as/g/2MQxiyNiKbC037JF/ebPAM4oVG1VsgyWLIGnn049kpqZdZAi90PeS2ofGEudxiNolGXp9tE77qi6EjOzYVd4PIJaa+xqYv/9q63FzGyYDRoEkn7Oq2/3fElEvKeUilrRrFkwbpzbCcysIzU7I/iXEaui1fUNUuOuJsysAw0aBBHRWh3AVS3L4Morq67CzGzYufOcorIM/vQn2LCh6krMzIaVg6AodzVhZh3KQVCUg8DMOpTvGipqt91gp50cBGbWcXzXUFGSu5ows47ku4Y2RZbBued6kBoz6yhFuqGeLemnklZJWt03jURxLSfL4Mkn4e67q67EzGzYFPlZ+z3gO8BG4G3A94ELmn6iU7nB2Mw6UJEg2DoirgYUEfdExD8Dh5RbVovad9/06iAwsw5SpBvqZySNAu7Kxxe4D9il3LJa1LbbpkFq3NWEmXWQImcEnwbGA/9AGl/4Y8CxJdbU2nznkJl1mCLdUC/L3z4BHF9uOW0gy+Dyy+GZZ1KPpGZmbW7IIJB0LQM8WBYR9WwnyDJ44YU0SM1++1VdjZnZFivSRvDZhvfjgPeT7iCqp8Y7hxwEZtYBilwaWt5v0W8l1fdhs9mzYaut3E5gZh2jyKWhHRtmR5EajHcrraJWN3o07L23g8DMOkaRS0PLSW0EIl0SWgP8TZlFtbwsg6uvrroKM7NhUSQI9o6IZxoXSNqqpHraQ5bBBRfAQw/BjjsOvb2ZWQsr8hzB7wZYdsNwF9JW3NWEmXWQZuMR7AZMAraWtB/p0hDAdqQHzOpr3rz0evvt8Ja3VFuLmdkWanZp6J3AccBk4OsNyx8HvlBiTa1v993TJSGfEZhZB2g2HsH5wPmS3h8Rl4xgTa2vb5Aa9zlkZh2gSGPxvpLm9l8YEV8poZ72kWVw3nkepMbM2l6Rv2BPAE/m0wvAYcD0EmtqD1kGTzwB99xTdSVmZlukyJPFZzbOS/oXYElpFbWLxjuHZsyothYzsy2wOdc0xgMzh7uQtuNBasysQxQZs/h2Sbfl00rgTuCbRXYuaYGkOyX1SPpck+0OkPSCpA8UL71iEybA9OkOAjNre0Uai9/V8H4j8EBEDNn7qKQu4CzgUKAXWCZpSUSsGmC7/wNcWbjqVuFBasysAwx6RiBpx7zDuccbpqeB7fp1RDeYA4GeiFgdEc8BFwFHDrDd3wOXAA9uavGVyzK480549tmqKzEz22zNzgg2kH7J9/36V8O6YOh2gknA2ob5XuD1jRtImgT8D+AQ4IDBdiRpIbAQYOrUqUN87QjqG6Tmj3+E17626mrMzDZLszaCbwEPA1eQxiieGREz8qlIY7EGWNZ/pLNvAKdExAvNdhQRiyNifkTM7+7uLvDVI8R9DplZB2j2ZPHJkgS8lTRg/bck/RL4TkSsKbDvXmBKw/xkYF2/beYDF6WvYWfgcEkbI+LfCx9BlfbcE8aOdRCYWVtr2lgcEQFcK+lm4GjgNOAu4OwC+14GzJY0A7gv//xH+u3/pRvwJZ0HXN42IQAwZkwapMZdTZhZG2vW++g2pMbdo4Bu4FJg/4hYO9hnGkXERkknke4G6gLOjYiVkk7I1y/a0uJbQpbBtddWXYWZ2WZrdkbwIOnX/4+AHtL1/QMkHQAQEZcOtfOIWAos7bdswACIiOOKldxisgx+8AN4+GHYYYeqqzEz22TNguBi0h//OfnUKEhnCNbYYPzmN1dbi5nZZmjWWHzcCNbRvhwEZtbm3H/ylpo0CSZO9J1DZta2HARbqm+QGgeBmbUpB8FwyDJYsQKi//NyZmatr0jvo+MlnSrp7Hx+tqR3DfW5WskyeOwxuPfeqisxM9tkRc4Ivgc8Cxycz/cC/6u0itqRu5owszZWJAj2iIivAc8DRMTTDNyPUH15kBoza2NFguA5SVuTdxgnaQ/SGYL12X57mDbNXU2YWVsqMjDNP5N6IJ0i6YfAG4DjSqypPfnOITNrU0UGr/+lpOXAQaRLQidHxIbSK2s3WQZXXAHPPZd6JDUzaxNF7hpaArwDuC4iLncIDCLLYOPGNEiNmVkbKdJGcCbwJmCVpIslfUDSuJLraj++c8jM2lSRS0PXA9fng8wfAvwtcC6wXcm1tZe99krjEzgIzKzNFGksJr9r6N2ksQn2B84vs6i2NGYMzJnjIDCztjNkEEj6MWnQ+SuAs0htBS+WXVhbyjL4zW+qrsLMbJMUfbJ4j4g4ISKucQg0kWWwdi088kjVlZiZFdZsqMpDIuIaYDxwZD7A/EuKjFBWO30NxitWwBvfWG0tZmYFNbs09BbgGlLbQH8eoWwg8+al19tvdxCYWdtoNkLZl/K3X4mINY3rJM0otap2NXly6m7CXU2YWRsp0kZwyQDLfjrchXQED1JjZm2oWRvBHGAusL2k9zWs2g7wA2WDyTK48MI0SI3cSauZtb5mbQR7Ae8CJvLKdoLHSQ+V2UCyDB59NN09NHVq1dWYmQ2pWRvBZcBlkg6OiBtGsKb21tjVhIPAzNpAkSeLb5Z0Iuky0UuXhCLi46VV1c4aB6k54ohqazEzK6BIY/EFwG7AO4Hrgcmky0M2kIkTYcoUNxibWdsoEgSzIuJU4MmIOB84AsjKLavN+c4hM2sjRYLg+fz1EUn7AtsD00urqBNkWRqX4Pnnh97WzKxiRYJgsaQdgFOBJcAq4GulVtXusiyFwJ13Vl2JmdmQioxHcE7+9npgZrnldIjGO4f6Go/NzFpUswfKPtPsgxHx9aF2LmkB8E2gCzgnIk7vt/6jwCn57BPAJyPi1qH22/LmzIHRo1NXEx/+cNXVmJk11eyMYMKW7Dgf0ews4FCgF1gmaUlErGrYbA3wloh4WNJhwGLS2AftbexYD1JjZm2j2QNlX97CfR8I9ETEagBJFwFHktoY+r7jdw3b30i6NbUzZBn89rdVV2FmNqQhG4sl7Snpakkr8vl5kr5YYN+TgLUN8735ssH8DfCLQWpYKOkmSTetX7++wFe3gCyDe+9N3U2YmbWwIncNnQ18nvw20oi4DTi6wOcG6nEtBtxQehspCE4ZaH1ELI6I+RExv7u7u8BXt4DGQWrMzFpYkSAYHxG/77dsY4HP9QJTGuYnA+v6byRpHnAOcGRE/LnAfttD451DZmYtrEgQbJC0B/mveUkfAO4v8LllwGxJMySNJZ1FLGncQNJU0khnH4uI/9qkylvd1Kmw3XYOAjNreUU6nTuRdDfPHEn3ke70+ehQH4qIjZJOAq4k3T56bkSslHRCvn4R8D+BnYB/y8dE3hgR8zfrSFqNlJ4hcBCYWYsr8kDZauCvJG1DOoN4GjgKuKfAZ5cCS/stW9Tw/hPAJzax5vaRZfDjH3uQGjNraYNeGpK0naTPS/q2pEOBp4BjgR7gQyNVYFvLMnjkEbjvvqorMTMbVLMzgguAh4EbSCOS/RMwFnhvRNxSfmkdoLHBeHLnPCJhZp2lWRDMjIgMQNI5wAZgakR4LIKiGoPgsMOqrcXMbBDN7hp6qQ/liHgBWOMQ2EQ77JDOBG67repKzMwG1eyM4LWSHsvfC9g6nxcQEbFd6dV1Ag9SY2YtrllfQ10jWUjHyjL41a/S+ARjxlRdjZnZqxR5oMy2RN8gNf/VWc/LmVnncBCUzV1NmFmLcxCUbc4c6OqCX/4SXnyx6mrMzF7FQVC2rbaCj34Uvvc9ePvbYc2aqisyM3sFB8FIOO88OPtsWL48XSpatCh1O2Fm1gIcBCNBgk98Io1NcPDB8MlPwjveAfcM2V2TmVnpHAQjaerU1FawaBHceGM6Ozj7bJ8dmFmlHAQjTYK/+7t0F9EBB8DChbBgAaxdO/RnzcxK4CCoyvTpcNVVcNZZaZD7ffeF737XZwdmNuIcBFUaNQo+9anUF9F++6V2hMMPh97eqiszsxpxELSCmTPhmmvgW9+CX/86nR2cd57PDsxsRDgIWsWoUXDSSensYN48OP54ePe7Yd26qiszsw7nIGg1e+wB110H3/hGOkuYOxcuuMBnB2ZWGgdBKxo1Ck4+GW69NQXBMcfAe98Lf/pT1ZWZWQdyELSy2bPh+uvhzDPT8wdz58KFF/rswMyGlYOg1XV1wWc+A7fcAnvumfotev/74YEHqq7MzDqEg6Bd7LUX/Od/whlnwNKl6ezgxz/22YGZbTEHQTvp6oLPfhZuvjk1Kh99NHzwg/Dgg1VXZmZtzEHQjvbeOz2NfPrp8POfp7ODiy+uuioza1MOgnY1ejSccgr84Q+pu4oPfQiOOgo2bKi6MjNrMw6Cdjd3LtxwA3z1q/Czn6X5Sy+tuiozayMOgk4wejR84Qtp4JvJk9NdRR/5CPz5z1VXZmZtwEHQSbIsjXNw2mnw05+ms4Mf/ABWr4aNG6uuzsxa1OiqC7BhNmYMfPGL8J73wLHHwsc+9vLyGTNg1qz0oNqsWS9P06enswozqyX/39+p5s2D3/8+nSHcdVeaenrSdP318OSTL287enQKg/4hMXt2Wj5mTFVHYWYjoNQgkLQA+CbQBZwTEaf3W698/eHAU8BxEfGHMmuqlTFj4E1vSlOjiPRkck/PKwPirrvSQ2tPPPHytl1dMG3aq88iZs9OZxhjx47sMZnZsCstCCR1AWcBhwK9wDJJSyJiVcNmhwGz8+n1wHfyVyuTBLvtlqY3vvGV6yLSA2qN4dD3/oYb4LHHXt521Kg0DnP/s4hZs1JIjBs3ssdVpog0vfhi82mgJ70He/p7uJYPB6m8zzZbX+ZnO9FWW8H48cO+2zLPCA4EeiJiNYCki4AjgcYgOBL4fkQEcKOkiZJ2j4j7S6zLmpFg113T9IY3vHJdRHpOYaAziQsvhEcffeX2Y8ak/UkpNPreDzRfdFnRzw31B3tTJ3flYa3glFPSg6TDrMwgmAQ0jsjey6t/7Q+0zSTgFUEgaSGwEGDq1KnDXqgVJEF3d5oOPviV6yLgoYdeDoj//m945pmXf0n3/ZpuNl90WZFturpSQIzk1BdKg/2zK3P5ltiSkBvqs83Wl/nZTjV/fim7LTMIBvovtv+/vSLbEBGLgcUA8+fPr+l/AS1Ogp12StNBB1VdjZltgjKfI+gFpjTMTwb6j7tYZBszMytRmUGwDJgtaYakscDRwJJ+2ywBjlFyEPCo2wfMzEZWaZeGImKjpJOAK0m3j54bESslnZCvXwQsJd062kO6ffT4suoxM7OBlfocQUQsJf2xb1y2qOF9ACeWWYOZmTXnvobMzGrOQWBmVnMOAjOzmnMQmJnVnKLNntCTtB64ZzM/vjNQt7Ecfcz14GOuhy055mkR0T3QirYLgi0h6aaIKOcZ7RblY64HH3M9lHXMvjRkZlZzDgIzs5qrWxAsrrqACviY68HHXA+lHHOt2gjMzOzV6nZGYGZm/TgIzMxqrjZBIGmBpDsl9Uj6XNX1lEHSuZIelLSiYdmOkq6SdFf+ukOVNQ43SVMkXSvpDkkrJZ2cL+/I45Y0TtLvJd2aH++X8+UdebyNJHVJulnS5fl8Rx+zpLsl3S7pFkk35ctKOeZaBIGkLuAs4DBgH+DDkvaptqpSnAcs6Lfsc8DVETEbuDqf7yQbgX+MiL2Bg4AT83+3nXrczwKHRMRrgdcBC/KxPDr1eBudDNzRMF+HY35bRLyu4dmBUo65FkEAHAj0RMTqiHgOuAg4suKahl1E/Bp4qN/iI4Hz8/fnA+8dyZrKFhH3R8Qf8vePk/5QTKJDjzuSJ/LZMfkUdOjx9pE0GTgCOKdhcUcf8yBKOea6BMEkYG3DfG++rA527Rv1LX/dpeJ6SiNpOrAf8P/o4OPOL5HcAjwIXBURHX28uW8A/wS82LCs0485gF9KWi5pYb6slGMudWCaFqIBlvm+2Q4iaVvgEuDTEfGYNNC/8s4QES8Ar5M0EfiZpH0rLqlUkt4FPBgRyyW9teJyRtIbImKdpF2AqyT9sawvqssZQS8wpWF+MrCuolpG2gOSdgfIXx+suJ5hJ2kMKQR+GBGX5os7/rgj4hHgOlK7UCcf7xuA90i6m3RZ9xBJP6Czj5mIWJe/Pgj8jHSJu5RjrksQLANmS5ohaSxwNLCk4ppGyhLg2Pz9scBlFdYy7JR++n8XuCMivt6wqiOPW1J3fiaApK2BvwL+SIceL0BEfD4iJkfEdNL/u9dExF/TwccsaRtJE/reA+8AVlDSMdfmyWJJh5OuM3YB50bEV6utaPhJ+hHwVlJXtQ8AXwL+HfgJMBW4F/hgRPRvUG5bkt4I/Aa4nZevH3+B1E7QccctaR6pkbCL9EPuJxHxFUk70YHH219+aeizEfGuTj5mSTNJZwGQLuFfGBFfLeuYaxMEZmY2sLpcGjIzs0E4CMzMas5BYGZWcw4CM7OacxCYmdWcg8BsGEiaKOlTVddhtjkcBGbDYyLgILC25CAwGx6nA3vkfcefUXUxZpvCD5SZDYO859PLI6KjO4CzzuQzAjOzmnMQmJnVnIPAbHg8DkyougizzeEgMBsGEfFn4LeSVrix2NqNG4vNzGrOZwRmZjXnIDAzqzkHgZlZzTkIzMxqzkFgZlZzDgIzs5pzEJiZ1dz/B6cymNKjMMvpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(0, 55, 5), list(relative_mutual_informations), c=\"red\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"Relative Mutual Information\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $t$ increases, the relative mutual information decreases. This makes sense as we can think of mutual information as a measure of the mutual dependence between two random variables, in our case $X(\\tau+t)$ and $X(\\tau)$. When $t=0$ we have the same random variable so a relative mutual information of $1$ makes sense. As $t$ increases, $X(\\tau+t)$ and $X(\\tau)$ say less information about each other, primarily due to the markov property.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 {-}\n",
    "\n",
    "**This problem and problem 3 below are loosely based on a homework in Eric Xing’s Graphical Models class at Carnegie Mellon: http://www.cs.cmu.edu/~epxing/Class/10708/**\n",
    "\n",
    "**Restricted Boltzmann machines (RBM) are a type of undirected graphical models in which the vertices are split into two layers. An example of an RBM is given by graph shown in Figure 1 which consists of vertices separated into a bottom layer $V_1, V_2, . . . , V_5$ and top layer $H_1, H_2$. Vertices in the bottom layer represent movies, e.g. Finding Nemo ($V_1$), Avatar ($V_2$), Star Trek ($V_3$), Aladdin ($V_4$), and Frozen ($V_5$). While referring to the nodes as $V_i$, we will also let $V_i$ be random variables taking values $0$ (movie disliked) and $1$ (movie was liked). The two vertices in the top layer represent latent factors (e.g., $H_1$ might be associated with Disney movies, and $H_2$ could represent the adventure genre). And as for the $V_i$, the $H_i$ are random variables taking values $0$ and $1$.**\n",
    "\n",
    "**The joint distribution of the random variables is given by**\n",
    "\n",
    "$$P(V=v, H=h)=\\alpha e^{E(v,h)}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~(1)$$ \n",
    "\n",
    "**where** \n",
    "\n",
    "$$E(v,h)=\\sum_{i=1}^{5}\\sum_{j=1}^{2}w_{ij}v_ih_j+\\sum_{i=1}^{5}a_iv_i+\\sum_{j=1}^{2}b_jh_j$$\n",
    "\n",
    "**and $V = (V_1,V_2,...,V_5), v = (v_1,v_2,...,v_5)$ with each $v_i \\in \\{0, 1 \\}$ and similarly for $H, h$. ($E(v, h)$ is referred to as the Energy function in analogy with Boltzmann distributions from statistical physics).**\n",
    "\n",
    "**The graph of the Figure below and equation (1) define a RBM.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 1: Restricted Boltzmann machine (RBM)](RBM_figure.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a {-}\n",
    "\n",
    "**Show that the RBM defined above is consistent with the network given in the Figure by checking that every pair of vertices not connected by an edge is conditionally independent. (Hint: Use equation (1) to express $P(V_1 = v_1, V_2 = v_2|V_3,V_4,V_5,H_1,H_2)$ and show that the resulting expression splits into a product of two factors dependent on $v_1$ and $v_2$, respectively.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show $P(V_1 = v_1, V_2 = v_2|V_3,V_4,V_5,H_1,H_2)=f(v_1)g(v_2)$.\n",
    "\n",
    "Note,\n",
    "\n",
    "$$P(V_1 = v_1, V_2 = v_2|V_3,V_4,V_5,H_1,H_2)$$\n",
    "\n",
    "$$=\\frac{P(V_1=v_1, V_2 = v_2, V_3,V_4,V_5,H_1,H_2)}{P(V_3, V_4, V_5, H_1, H_2)}$$\n",
    "\n",
    "$$=\\frac{\\alpha e^{\\sum_{i=1}^{5}\\sum_{j=1}^{2}w_{ij}v_ih_j+\\sum_{i=1}^{5}a_iv_i+\\sum_{j=1}^{2}b_jh_j}}{C}$$\n",
    "\n",
    "$$=\\frac{\\alpha e^{(v_1\\sum_{j=1}^{2}w_{1j}H_j+a_1v_1)+(v_2\\sum_{j=1}^{2}w_{2j}H_j+a_2v_2)+\\sum_{i=3}^{5}\\sum_{j=1}^{2}w_{ij}V_iH_j+\\sum_{i=3}^{5}a_iV_i+\\sum_{j=1}^{2}b_jH_j}}{C}$$\n",
    "\n",
    "$$=\\frac{\\alpha e^{v_1\\sum_{j=1}^{2}w_{1j}H_j+a_1v_1}e^{v_2\\sum_{j=1}^{2}w_{2j}H_j+a_2v_2}e^{\\sum_{i=3}^{5}\\sum_{j=1}^{2}w_{ij}V_iH_j+\\sum_{i=3}^{5}a_iV_i+\\sum_{j=1}^{2}b_jH_j}}{C}$$\n",
    "\n",
    "$$=\\frac{\\alpha e^{v_1\\sum_{j=1}^{2}w_{1j}H_j+a_1v_1}e^{v_2\\sum_{j=1}^{2}w_{2j}H_j+a_2v_2}C'}{C}$$\n",
    "\n",
    "as the last exponential term does not depend on $v_1, v_2$\n",
    "\n",
    "$$=\\big [\\frac{C'\\alpha}{C}e^{v_1\\sum_{j=1}^{2}w_{1j}H_j+a_1v_1}\\big ]\\big [e^{v_2\\sum_{j=1}^{2}w_{2j}H_j+a_2v_2} \\big ]$$\n",
    "\n",
    "$$=f(v_1)g(v_2)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remaining pairs $v_i$ that are not connected by an edge such as $V_1, V_3$ or $V_4, V_5$ or $V_2, V_5$, etc, by symmetry, the same argument applies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish showing consistency, we have to show $P(H_1 = h_1, H_2 = h_2|V_1, V_2, V_3,V_4,V_5)=f(h_1)g(h_2)$:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(H_1 = h_1, H_2 = h_2|V_1, V_2, V_3,V_4,V_5)$$\n",
    "\n",
    "$$=\\frac{P(H_1 = h_1, H_2 = h_2|V_1, V_2, V_3,V_4,V_5)}{P(V_1, V_2, V_3, V_4, V_5)}$$\n",
    "\n",
    "$$=\\frac{\\alpha e^{\\sum_{i=1}^{5}\\sum_{j=1}^{2}w_{ij}v_ih_j+\\sum_{i=1}^{5}a_iv_i+\\sum_{j=1}^{2}b_jh_j}}{C}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$=\\frac{\\alpha e^{\\sum_{i=1}^{5}w_{ij}V_ih_1 + \\sum_{i=1}^{5}w_{ij}V_ih_2 + \\sum^{5}_{i=1}a_iV_i + b_1h_1+b_2h_2}}{C}$$\n",
    "\n",
    "$$=\\frac{\\alpha e^{(\\sum_{i=1}^{5}w_{ij}V_ih_1+ b_1h_1) + (\\sum_{i=1}^{5}w_{ij}V_ih_2+b_2h_2) + \\sum^{5}_{i=1}a_iV_i}}{C}$$\n",
    "\n",
    "$$=\\frac{\\alpha e^{(\\sum_{i=1}^{5}w_{ij}V_ih_1+ b_1h_1)}e^{(\\sum_{i=1}^{5}w_{ij}V_ih_2+b_2h_2)}C'}{C}$$\n",
    "\n",
    "$$=\\big [\\frac{\\alpha C'}{C} e^{\\sum_{i=1}^{5}w_{ij}V_ih_1+ b_1h_1}\\big ] \\big [e^{\\sum_{i=1}^{5}w_{ij}V_ih_2+b_2h_2}\\big ]$$\n",
    "\n",
    "$$=f(h_1)g(h_2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b {-}\n",
    "\n",
    "**Let $V_{−1}$ be all $V_i$ without $V_1$, i.e. $V_{−1} = (V_2,V_3,...,V_5)$. Show that**\n",
    "\n",
    "$$P(V_1=v_1|V_{-1}, H)=\\frac{1}{A+B}$$\n",
    "\n",
    "**where $A, B$ are the expressions:**\n",
    "\n",
    "$$A=\\exp[\\sum_{j=1}^{2}w_{1j}(0-v_1)h_j+a_1(0-v_1)]$$\n",
    "\n",
    "$$B=\\exp[\\sum_{j=1}^{2}w_{1j}(1-v_1)h_j+a_1(1-v_1)]$$\n",
    "\n",
    "**where $\\exp(x)=e^x$. Generalize to calculate the conditional distribution of $V_i$ given all other vertices and $H_i$ given all other vertices.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first show $P(V_1=v_1|V_{-1}, H)=\\frac{1}{A+B}$.\n",
    "\n",
    "$$P(V_1=v_1|V_{-1}, H)$$\n",
    "\n",
    "$$=\\frac{P(V_1=v_1, V_{-1}, H)}{P(V_{-1},H)}$$\n",
    "\n",
    "$$=\\frac{P(V_1=v_1, V_{-1}, H)}{\\sum_{v_1=0}^{1}P(V_1=v_1, V_{-1}, H)}$$\n",
    "\n",
    "$$=\\frac{\\exp \\big [(v_1\\sum_{j=1}^{2}w_{1j}H_j+a_1v_1)+\\sum_{i=2}^{5}\\sum_{j=1}^{2}w_{ij}V_iH_j+\\sum_{i=2}^{5}a_iV_i+\\sum_{j=1}^{2}b_jH_j \\big ]}{\\sum_{v_1=0}^{1}\\exp \\big [(v_1\\sum_{j=1}^{2}w_{1j}H_j+a_1v_1)+\\sum_{i=2}^{5}\\sum_{j=1}^{2}w_{ij}V_iH_j+\\sum_{i=2}^{5}a_iV_i+\\sum_{j=1}^{2}b_jH_j \\big ]}$$\n",
    "\n",
    "$$=\\frac{\\exp \\big [ v_1\\sum_{j=1}^{2}w_{1j}H_j+a_1v_1\\big ]}{\\sum_{v_1=0}^{1}\\exp \\big [ v_1\\sum_{j=1}^{2}w_{1j}H_j+a_1v_1\\big ]}$$\n",
    "\n",
    "$$=\\frac{\\exp \\big [ v_1\\sum_{j=1}^{2}w_{1j}H_j+a_1v_1\\big ]}{\\exp \\big [ \\sum_{j=1}^{2}w_{1j}0H_j+a_10\\big ]+\\exp \\big [ \\sum_{j=1}^{2}w_{1j}1H_j+a_11\\big ]}$$\n",
    "\n",
    "Now, dividing both numerator and denominator by the value in the numerator, $\\exp \\big [ v_1\\sum_{j=1}^{2}w_{1j}H_j+a_1v_1\\big ]$:\n",
    "\n",
    "$$=\\frac{1}{\\exp \\big [ \\sum_{j=1}^{2}w_{1j}(0-v_1)H_j+a_1(0-v_1)\\big ]+\\exp \\big [ \\sum_{j=1}^{2}w_{1j}(1-v_1)H_j+a_1(1-v_1)\\big ]}$$\n",
    "\n",
    "$$=\\frac{1}{A+B}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By symmetry, we can generalize this to all $V_i$ for $i=1,2,3,4,5:$\n",
    "\n",
    "$$P(V_i=v_i|V_{-i}, H)$$\n",
    "\n",
    "$$=\\frac{1}{\\exp \\big [ \\sum_{j=1}^{2}w_{ij}(0-v_i)H_j+a_i(0-v_i)\\big ]+\\exp \\big [ \\sum_{j=1}^{2}w_{ij}(1-v_i)H_j+a_i(1-v_i)\\big ]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, by symmetry, we can generalize this to $H_j$ for $j=1,2$:\n",
    "\n",
    "$$P(H_j=h_j|H_{-j}, V)$$\n",
    "\n",
    "$$=\\frac{1}{\\exp \\big [ \\sum_{i=1}^{5}w_{ij}(0-h_j)V_i+b_j(0-h_j)\\big ]+\\exp \\big [ \\sum_{i=1}^{5}w_{ij}(1-h_j)V_i+b_j(1-h_j)\\big ]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c {-}\n",
    "\n",
    "**For the rest of this problem, assume $a_i = 0, b_i = 0$ and $w_{ij}$ is given by the following matrix:**\n",
    "\n",
    "$$W=\\begin{pmatrix}\n",
    "2 & -3 \\\\\n",
    "2 & -3 \\\\\n",
    "-2 & 3 \\\\\n",
    "-2 & -2 \\\\\n",
    "0 & 2\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "**Use your result from (b) to write a Gibbs sampler for the RBM.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2, -3],\n",
       "       [ 2, -3],\n",
       "       [-2,  3],\n",
       "       [-2, -2],\n",
       "       [ 0,  2]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "W = np.array([[2,-3], [2, -3], [-2, 3], [-2, -2], [0,2]])\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample $\\omega^{(i+1)}$ is obtained from $\\omega^{(i)}$ after obtaining each $V$ and each $H$ in a sequential manner using the respective marginal distributions (instead of considering each $\\omega^{(i+1)}$ to be from changing only one coordinate of $\\omega^{(i)}$). This will speed up convergence to the stationary distribution and decrease relaxation time.\n",
    "\n",
    "In other words:\n",
    "\n",
    "Given $\\omega^{(i)}=(V_1^{(i)}, V_2^{(i)}, V_3^{(i)}, V_4^{(i)}, V_5^{(i)}, H_1^{(i)}, H_2^{(i)})$\n",
    "\n",
    "- Sample $\\hat{V_1}^{(i+1)}$ from $P(V_1|V_2^{(i)}, V_3^{(i)}, V_4^{(i)}, V_5^{(i)}, H_1^{(i)}, H_2^{(i)})$\n",
    "\n",
    "- Sample $\\hat{V_2}^{(i+1)}$ from $P(V_2|V_1^{(i+1)}, V_3^{(i)}, V_4^{(i)}, V_5^{(i)}, H_1^{(i)}, H_2^{(i)})$\n",
    "\n",
    "- Sample $\\hat{V_3}^{(i+1)}$ from $P(V_3|V_1^{(i+1)}, V_2^{(i+1)}, V_4^{(i)}, V_5^{(i)}, H_1^{(i)}, H_2^{(i)})$\n",
    "\n",
    "- Sample $\\hat{V_4}^{(i+1)}$ from $P(V_4|V_1^{(i+1)}, V_2^{(i+1)}, V_3^{(i+1)}, V_5^{(i)}, H_1^{(i)}, H_2^{(i)})$\n",
    "\n",
    "- Sample $\\hat{V_5}^{(i+1)}$ from $P(V_5|V_1^{(i+1)}, V_2^{(i+1)}, V_3^{(i+1)}, V_4^{(i+1)}, H_1^{(i)}, H_2^{(i)})$\n",
    "\n",
    "- Sample $\\hat{H_1}^{(i+1)}$ from $P(H_1|V_1^{(i+1)}, V_2^{(i+1)}, V_3^{(i+1)}, V_4^{(i+1)}, V_5^{(i+1)}, H_2^{(i)})$\n",
    "\n",
    "- Sample $\\hat{H_2}^{(i+1)}$ from $P(H_2|V_1^{(i+1)}, V_2^{(i+1)}, V_3^{(i+1)}, V_4^{(i+1)}, V_5^{(i+1)}, H_1^{(i+1)})$\n",
    "\n",
    "And so, $\\omega^{(i+1)}=(V_1^{(i+1)}, V_2^{(i+1)}, V_3^{(i+1)}, V_4^{(i+1)}, V_5^{(i+1)}, H_1^{(i+1)}, H_2^{(i+1)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def V_marginal(i, omega, W):\n",
    "    \"\"\"Marginal for each Vi, i=1,2,3,4,5\"\"\"\n",
    "    p0 = 1 / (1 + np.exp(W[i,:].dot(omega[-2:])))\n",
    "    p1 = 1 / (1 + np.exp(-W[i,:].dot(omega[-2:])))\n",
    "    return [p0, p1]\n",
    "\n",
    "def H_marginal(j, omega, W):\n",
    "    \"\"\"Marginal for each Hi, i=1,2\"\"\"\n",
    "    p0 = 1 / (1 + np.exp(W[:,j].dot(omega[:-2])))\n",
    "    p1 = 1 / (1 + np.exp(-W[:,j].dot(omega[:-2])))\n",
    "    return [p0, p1]\n",
    "\n",
    "def Gibbs_Sampler(W, omega, N, sample_hidden=True, sample_observed=True):\n",
    "    \"\"\"sampling the joint prob of (V, H) via Gibbs Sampling\"\"\"\n",
    "    samples = [omega]\n",
    "    for n in range(0, N):\n",
    "        if sample_observed:\n",
    "            for i in range(0, 5):\n",
    "                omega[i] = np.random.choice([0, 1], p=V_marginal(i, omega, W))\n",
    "        if sample_hidden:\n",
    "            for j in range(5, 7):\n",
    "                omega[j] = np.random.choice([0, 1], p=H_marginal(j-5, omega, W))\n",
    "        samples.append(omega.copy())\n",
    "    return np.array(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d {-}\n",
    "\n",
    "**Calculate the relaxation time of your Gibbs sampler exactly. Remember, the Gibbs sampler is a special case of a MH sampler. Here you have $2^7$ possible states for your MH Markov chain and you can specify the transition probabilities, so you can calculate the transition probability matrix.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(\\omega^{(i)}|\\omega^{(i-1)})$ can be obtained from multiplying the probabilites listed in the bullet points above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function gets $P(\\omega^{(i)}|\\omega^{(i-1)})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_transition_prob(w, wprime):\n",
    "    prob = 1\n",
    "    for i in range(0, 5):\n",
    "        A_V = np.exp(np.multiply(W[i,:].dot(w[-2:]), 0-wprime[i]))\n",
    "        B_V = np.exp(np.multiply(W[i,:].dot(w[-2:]), 1-wprime[i]))\n",
    "        probs_V = 1 / (A_V + B_V)\n",
    "        prob = prob * probs_V\n",
    "\n",
    "    for j in range(5, 7):\n",
    "        A_H = np.exp(np.multiply(W.T[5-j].dot(wprime[:-2]), 0-wprime[j]))\n",
    "        B_H = np.exp(np.multiply(W.T[5-j].dot(wprime[:-2]), 1-wprime[j]))\n",
    "        probs_H = 1 / (A_H + B_H)\n",
    "        prob = prob * probs_H\n",
    "\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define all possible states, $2^7$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "states = np.array(list(product(range(2), repeat=7)))\n",
    "transition_prob_kernel = np.empty((2**7, 2**7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's get the transition probability matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, w in enumerate(states):\n",
    "    for j, wprime in enumerate(states):\n",
    "        transition_prob_kernel[i, j] = calculate_transition_prob(w, wprime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking that we have a transition probability kernel\n",
    "# rows must sum to 1\n",
    "transition_prob_kernel.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues = np.linalg.eig(transition_prob_kernel)[0]\n",
    "eigenvectors = np.linalg.eig(transition_prob_kernel)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.499766333682572"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvalue_2 = sorted(abs(eigenvalues), reverse=True)[1]\n",
    "rt = 1/(1-eigenvalue_2)\n",
    "rt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relaxation time for the gibbs sampler I defined is roughly 6.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e {-}\n",
    "\n",
    "**Use your Gibbs sampler to determine the distribution of $H_1$ and $H_2$ given that a user (a) likes the movies corresponding to $V_1$ and $V_2$ but dislikes the others and (b) likes all the movies.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Likes movies $V_1, V_2$, i.e., $V_1=V_2=1$ and dislikes movies $V_3,V_4, V_5$, i.e., $V_3=V_4=V_5=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10**6\n",
    "# fixing the first 5 entries of omega to what is described in a) which corresponds to the observed nodes V\n",
    "# initializing the hidden nodes to 0\n",
    "omega_start = np.array([1,1,0,0,0,0,0])\n",
    "samples = Gibbs_Sampler(W, omega_start, N, sample_hidden=True, sample_observed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_samples = np.array(samples)[:,-2:]\n",
    "# burn in time\n",
    "hidden_samples_wBurnIn = hidden_samples[int(0.3*N):,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(H_1=0, H_2=0|V)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "p00 = len(hidden_samples_wBurnIn[(hidden_samples_wBurnIn[:,0] == 0) & (hidden_samples_wBurnIn[:,1] == 0)])/len(hidden_samples_wBurnIn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017817117404117993"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(H_1=0, H_2=1|V)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "p01 = len(hidden_samples_wBurnIn[(hidden_samples_wBurnIn[:,0] == 0) & (hidden_samples_wBurnIn[:,1] == 1)])/len(hidden_samples_wBurnIn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.1428512244982505e-05"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(H_1=1, H_2=0|V)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "p10 = len(hidden_samples_wBurnIn[(hidden_samples_wBurnIn[:,0] == 1) & (hidden_samples_wBurnIn[:,1] == 0)])/len(hidden_samples_wBurnIn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9797371718040403"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(H_1=1, H_2=1|V)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "p11 = len(hidden_samples_wBurnIn[(hidden_samples_wBurnIn[:,0] == 1) & (hidden_samples_wBurnIn[:,1] == 1)])/len(hidden_samples_wBurnIn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0024042822795967435"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H1 = 0</th>\n",
       "      <th>H1 = 1</th>\n",
       "      <th>Marginal H2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>H2 = 0</th>\n",
       "      <td>0.017817</td>\n",
       "      <td>0.979737</td>\n",
       "      <td>0.997554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H2 = 1</th>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>0.002446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marginal H1</th>\n",
       "      <td>0.017859</td>\n",
       "      <td>0.982141</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               H1 = 0    H1 = 1  Marginal H2\n",
       "H2 = 0       0.017817  0.979737     0.997554\n",
       "H2 = 1       0.000041  0.002404     0.002446\n",
       "Marginal H1  0.017859  0.982141     1.000000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([[p00, p10, p00+p10], [p01, p11, p01+p11], [p00+p01, p10+p11, p00+p10+p01+p11]])\n",
    "df.columns =['H1 = 0', 'H1 = 1', 'Marginal H2'] \n",
    "df.index = ['H2 = 0', 'H2 = 1', 'Marginal H1'] \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Like movies $V_1, V_2, V_3,V_4, V_5$, i.e., $V_1=V_2=V_3=V_4=V_5=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing the first 5 entries of omega to what is described in b) which corresponds to the observed nodes V\n",
    "# initializing the hidden nodes to 0 \n",
    "omega_start = np.array([1,1,1,1,1,0,0])\n",
    "samples = Gibbs_Sampler(W, omega_start, N, sample_hidden=True, sample_observed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_samples = np.array(samples)[:,-2:]\n",
    "# burn in time\n",
    "hidden_samples_wBurnIn = hidden_samples[int(0.3*N):,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(H_1=0, H_2=0|V)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p00 = len(hidden_samples_wBurnIn[(hidden_samples_wBurnIn[:,0] == 0) & (hidden_samples_wBurnIn[:,1] == 0)])/len(hidden_samples_wBurnIn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47646503362138054"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(H_1=0, H_2=1|V)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p01 = len(hidden_samples_wBurnIn[(hidden_samples_wBurnIn[:,0] == 0) & (hidden_samples_wBurnIn[:,1] == 1)])/len(hidden_samples_wBurnIn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.023848537359232345"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(H_1=1, H_2=0|V)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "p10 = len(hidden_samples_wBurnIn[(hidden_samples_wBurnIn[:,0] == 1) & (hidden_samples_wBurnIn[:,1] == 0)])/len(hidden_samples_wBurnIn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4761193198295431"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(H_1=1, H_2=1|V)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "p11 = len(hidden_samples_wBurnIn[(hidden_samples_wBurnIn[:,0] == 1) & (hidden_samples_wBurnIn[:,1] == 1)])/len(hidden_samples_wBurnIn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.023567109189844015"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H1 = 0</th>\n",
       "      <th>H1 = 1</th>\n",
       "      <th>Marginal H2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>H2 = 0</th>\n",
       "      <td>0.476465</td>\n",
       "      <td>0.476119</td>\n",
       "      <td>0.952584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H2 = 1</th>\n",
       "      <td>0.023849</td>\n",
       "      <td>0.023567</td>\n",
       "      <td>0.047416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marginal H1</th>\n",
       "      <td>0.500314</td>\n",
       "      <td>0.499686</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               H1 = 0    H1 = 1  Marginal H2\n",
       "H2 = 0       0.476465  0.476119     0.952584\n",
       "H2 = 1       0.023849  0.023567     0.047416\n",
       "Marginal H1  0.500314  0.499686     1.000000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([[p00, p10, p00+p10], [p01, p11, p01+p11], [p00+p01, p10+p11, p00+p10+p01+p11]])\n",
    "df.columns =['H1 = 0', 'H1 = 1', 'Marginal H2'] \n",
    "df.index = ['H2 = 0', 'H2 = 1', 'Marginal H1'] \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 {-}\n",
    "\n",
    "**Now we will consider the RBM of problem 2 again, but this time focusing on inferring the parameters. We will assume throughout that $a_i =0$ and $b_i =0$. Let $\\theta=(w_{11},w_{12},w_{21},w_{22},...,w_{51},w_{52})$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a {-}\n",
    "\n",
    "**Using the values of the parameters given in part (c) of problem 2, generate $N = 100$ independent samples of the random variables $(V, H)$ in the RBM. That is, you will sample $(V,H)~ 100$ times, so that each $(V,H)$ sample is independent of the others.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "indep_samples = np.empty((100, 7))\n",
    "for i in range(0, 100):\n",
    "    omega_start = np.random.randint(2, size=7)\n",
    "    sample = Gibbs_Sampler(W, omega_start, 10**4, sample_hidden=True, sample_observed=True)[-1]\n",
    "    indep_samples[i, :] = sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b {-}\n",
    "\n",
    "**Let $\\hat{V}^{(k)}$ and $\\hat{H}^{(k)}$ for $k = 1,2,...,100$ be the iid samples you generated in (a). Let $l(\\theta)$ be the log-likelihood of this data. Show**\n",
    "\n",
    "$$l(\\theta)=N\\log(\\alpha(\\theta))+\\sum_{k=1}^{100}\\sum_{i=1}^{5}\\sum_{j=1}^{2}w_{ij}\\hat{V}_i^{(k)}\\hat{H}_j^{(k)}$$\n",
    "\n",
    "**and that**\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial w_{mn}}(\\theta)=-NE_{\\theta}[V_mH_n]+\\sum_{k=1}^{N}\\hat{V}_m^{(k)}\\hat{H}_n^{(k)}$$\n",
    "\n",
    "**where $E_{\\theta}[V_mH_n]$ is the expected value of $V_mH_n$ under the parameter $\\theta$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the prompt, we know that $P(\\hat{V}, \\hat{H}; \\theta)=\\alpha e^{\\sum_{i=1}^{5}\\sum_{j=1}^{2}w_{ij}\\hat{V}_i\\hat{H}_j}$ where $\\theta=(w_{11},w_{12},w_{21},w_{22},...,w_{51},w_{52})$\n",
    "\n",
    "Let $\\hat{V}^{(k)}$ and $\\hat{H}^{(k)}$ for $k = 1,2,...,100$ be iid. Then the likelihood of this data is,\n",
    "\n",
    "$$L(\\theta)$$\n",
    "\n",
    "$$=\\prod_{k=1}^{100}P(\\hat{V}, \\hat{H}; \\theta)$$\n",
    "\n",
    "$$=\\prod_{k=1}^{100}\\alpha e^{\\sum_{i=1}^{5}\\sum_{j=1}^{2}w_{ij}\\hat{V}_i^{(k)}\\hat{H}_j^{(k)}}$$\n",
    "\n",
    "$$=\\alpha^{100} e^{\\sum_{k=1}^{100}\\sum_{i=1}^{5}\\sum_{j=1}^{2}w_{ij}\\hat{V}_i^{(k)}\\hat{H}_j^{(k)}}$$\n",
    "\n",
    "And so the log-likelihood of the data is,\n",
    "\n",
    "$$l(\\theta)=\\log L(\\theta)$$\n",
    "\n",
    "$$\\log \\alpha^{100} e^{\\sum_{k=1}^{100}\\sum_{i=1}^{5}\\sum_{j=1}^{2}w_{ij}\\hat{V}_i^{(k)}\\hat{H}_j^{(k)}}$$\n",
    "\n",
    "$$=100\\log \\alpha + \\sum_{k=1}^{100}\\sum_{i=1}^{5}\\sum_{j=1}^{2}w_{ij}\\hat{V}_i^{(k)}\\hat{H}_j^{(k)}$$\n",
    "\n",
    "Now, by noting that $\\alpha$ must be such that \n",
    "\n",
    "$$\\sum_{\\hat{V_1}=0}^{1}\\sum_{\\hat{V_1}=0}^{1}\\ldots \\sum_{\\hat{V_5}=0}^{1}\\sum_{\\hat{H_1}=0}^{1}\\sum_{\\hat{H_2}=0}^{1}\\alpha e^{\\sum_{i=1}^{5}\\sum_{j=1}^{2}w_{ij}\\hat{V}_i\\hat{H}_j}=1$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\implies \\alpha = \\frac{1}{\\sum_{\\hat{V_1}=0}^{1}\\sum_{\\hat{V_1}=0}^{1}\\ldots \\sum_{\\hat{V_5}=0}^{1}\\sum_{\\hat{H_1}=0}^{1}\\sum_{\\hat{H_2}=0}^{1}e^{\\sum_{i=1}^{5}\\sum_{j=1}^{2}w_{ij}\\hat{V}_i\\hat{H}_j}}$$\n",
    "\n",
    "and so $\\alpha$ is indeed a function of $\\theta$. Therefore, we have,\n",
    "\n",
    "$$l(\\theta)$$\n",
    "\n",
    "$$=100\\log \\alpha(\\theta) + \\sum_{k=1}^{100}\\sum_{i=1}^{5}\\sum_{j=1}^{2}w_{ij}\\hat{V}_i^{(k)}\\hat{H}_j^{(k)}$$\n",
    "\n",
    "where $100=N$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's derive \n",
    "\n",
    "$$\\frac{\\partial l}{\\partial w_{mn}}(\\theta)$$\n",
    "\n",
    "$$=\\frac{\\partial}{\\partial w_{mn}}(100\\log \\alpha(\\theta) + \\sum_{k=1}^{100}\\sum_{i=1}^{5}\\sum_{j=1}^{2}w_{ij}\\hat{V}_i^{(k)}\\hat{H}_j^{(k)})\n",
    "$$\n",
    "\n",
    "$$=100\\frac{\\partial}{\\partial w_{mn}}\\log \\alpha(\\theta) + \\frac{\\partial}{\\partial w_{mn}}\\sum_{k=1}^{100}\\sum_{i=1}^{5}\\sum_{j=1}^{2}w_{ij}\\hat{V}_i^{(k)}\\hat{H}_j^{(k)}$$\n",
    "\n",
    "Let's first derive $\\frac{\\partial}{\\partial w_{mn}}\\log \\alpha(\\theta)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First note,\n",
    "\n",
    "$$\\log \\alpha(\\theta)$$\n",
    "$$=\\log\\frac{1}{\\sum_{\\hat{V_1}=0}^{1}\\sum_{\\hat{V_1}=0}^{1}\\ldots \\sum_{\\hat{V_5}=0}^{1}\\sum_{\\hat{H_1}=0}^{1}\\sum_{\\hat{H_2}=0}^{1}e^{\\sum_{i=1}^{5}\\sum_{j=1}^{2}w_{ij}\\hat{V}_i\\hat{H}_j}}$$\n",
    "$$=-\\log \\sum_{\\hat{V_1}=0}^{1}\\sum_{\\hat{V_1}=0}^{1}\\ldots \\sum_{\\hat{V_5}=0}^{1}\\sum_{\\hat{H_1}=0}^{1}\\sum_{\\hat{H_2}=0}^{1}e^{\\sum_{i=1}^{5}\\sum_{j=1}^{2}w_{ij}\\hat{V}_i\\hat{H}_j}$$\n",
    "\n",
    "So,\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w_{mn}}\\log \\alpha(\\theta)$$\n",
    "\n",
    "$$=\\frac{\\partial}{\\partial w_{mn}}(-\\log \\sum_{\\hat{V_1}=0}^{1}\\sum_{\\hat{V_1}=0}^{1}\\ldots \\sum_{\\hat{V_5}=0}^{1}\\sum_{\\hat{H_1}=0}^{1}\\sum_{\\hat{H_2}=0}^{1}e^{\\sum_{i=1}^{5}\\sum_{j=1}^{2}w_{ij}\\hat{V}_i\\hat{H}_j})$$\n",
    "\n",
    "$$=- \\frac{\\frac{\\partial}{\\partial w_{mn}}( \\sum_{\\hat{V_1}=0}^{1}\\sum_{\\hat{V_1}=0}^{1}\\ldots \\sum_{\\hat{V_5}=0}^{1}\\sum_{\\hat{H_1}=0}^{1}\\sum_{\\hat{H_2}=0}^{1}e^{\\sum_{i=1}^{5}\\sum_{j=1}^{2}w_{ij}\\hat{V}_i\\hat{H}_j})}{\\sum_{\\hat{V_1}=0}^{1}\\sum_{\\hat{V_1}=0}^{1}\\ldots \\sum_{\\hat{V_5}=0}^{1}\\sum_{\\hat{H_1}=0}^{1}\\sum_{\\hat{H_2}=0}^{1}e^{\\sum_{i=1}^{5}\\sum_{j=1}^{2}w_{ij}\\hat{V}_i\\hat{H}_j}} $$\n",
    "\n",
    "$$=-\\frac{\\sum_{\\hat{V_1}=0}^{1}\\sum_{\\hat{V_1}=0}^{1}\\ldots \\sum_{\\hat{V_5}=0}^{1}\\sum_{\\hat{H_1}=0}^{1}\\sum_{\\hat{H_2}=0}^{1}\\hat{V}_m\\hat{H}_ne^{\\sum_{i=1}^{5}\\sum_{j=1}^{2}w_{ij}\\hat{V}_i\\hat{H}_j})}{\\sum_{\\hat{V_1}=0}^{1}\\sum_{\\hat{V_1}=0}^{1}\\ldots \\sum_{\\hat{V_5}=0}^{1}\\sum_{\\hat{H_1}=0}^{1}\\sum_{\\hat{H_2}=0}^{1}e^{\\sum_{i=1}^{5}\\sum_{j=1}^{2}w_{ij}\\hat{V}_i\\hat{H}_j}}$$\n",
    "\n",
    "$$=-\\sum_{\\hat{V_1}=0}^{1}\\sum_{\\hat{V_1}=0}^{1}\\ldots \\sum_{\\hat{V_5}=0}^{1}\\sum_{\\hat{H_1}=0}^{1}\\sum_{\\hat{H_2}=0}^{1}\\hat{V}_m\\hat{H}_nP(\\hat{V}, \\hat{H}; \\theta)$$\n",
    "\n",
    "$$=-E_{\\theta}[V_mH_n]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly,\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w_{mn}}\\sum_{k=1}^{100}\\sum_{i=1}^{5}\\sum_{j=1}^{2}w_{ij}\\hat{V}_i^{(k)}\\hat{H}_j^{(k)}$$\n",
    "\n",
    "$$=\\sum_{k=1}^{100}\\hat{V}_m^{(k)}\\hat{H}_n^{(k)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so putting everything together:\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial w_{mn}}(\\theta)$$\n",
    "\n",
    "$$=100\\frac{\\partial}{\\partial w_{mn}}\\log \\alpha(\\theta) + \\frac{\\partial}{\\partial w_{mn}}\\sum_{k=1}^{100}\\sum_{i=1}^{5}\\sum_{j=1}^{2}w_{ij}\\hat{V}_i^{(k)}\\hat{H}_j^{(k)}$$\n",
    "\n",
    "$$=-100E_{\\theta}[V_mH_n] + \\sum_{k=1}^{100}\\hat{V}_m^{(k)}\\hat{H}_n^{(k)}$$\n",
    "\n",
    "where $100=N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c {-}\n",
    "\n",
    "**Use a steepest ascent iteration**\n",
    "\n",
    "$$\\theta^{(i+1)}=\\theta^{(i)}+\\epsilon\\nabla l(\\theta^{(i)})$$\n",
    "\n",
    "**to find the MLE for $\\theta$. Make $\\epsilon$ small so backtracking will not be needed. At each iteration use your Gibbs sampler to calculate the expression $E_{\\theta}[V_mH_n]$. Note you can run your Gibbs sampler once to generate this expected value for all pairs $m, n$. Also, at each iteration $i$ of the steepest ascent algorithm, save the last state of your Gibbs sampler and use it as the initial state in iteration $i+1$ of the steepest ascent algorithm. This will greatly reduce the number of time steps you need to run the Gibbs sampler. Compare your MLE to the actual value of $\\theta$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below estimates $E_{\\theta}[V_mH_n]$ when passing samples generated via Gibbs Sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_value_estimate(samples, m, n):\n",
    "    return np.mean(np.sum(samples[:,m]*samples[:,n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below calculates the gradient as specified in b) and in the prompt for c):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient(omega_start, W):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        omega_start: last state of Gibbs sampler from previous iteration\n",
    "        W: current values of parameters\n",
    "    Returns:\n",
    "        gradient: value of gradient\n",
    "        samples[-1]: last state of Gibbs sampler   \n",
    "    \"\"\"\n",
    "    samples = Gibbs_Sampler(W, omega_start, 10**3, sample_hidden=True, sample_observed=True)\n",
    "    gradient = np.empty((10))\n",
    "    i = 0\n",
    "    for m in range(0,5):\n",
    "        for n in range(0,2):\n",
    "            gradient[i] = -100*expected_value_estimate(samples, m, n) + np.sum(indep_samples[:,m]*indep_samples[:,n])\n",
    "            i = i + 1\n",
    "    return gradient, samples[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepest_ascent(N, theta, omega_start, eps=0.000001):\n",
    "    for n in tqdm(range(0, N)):\n",
    "        gradient, omega_start = get_gradient(omega_start, theta.reshape(5,2)) \n",
    "        theta = theta + eps*gradient\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# warm start for the value of omega_start to pass to the first iteration of gradient ascent\n",
    "theta_start = np.random.randint(2, size=10)\n",
    "omega_start = Gibbs_Sampler(theta_start.reshape(5,2), np.random.randint(2, size=7), 10**5, sample_hidden=True, sample_observed=True)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:23<00:00,  4.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-4.0237, -1.8394],\n",
       "       [-1.8394, -4.1094],\n",
       "       [-1.0547, -2.1016],\n",
       "       [-1.8794, -1.8962],\n",
       "       [-1.1629, -1.1858]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm # timer in secs\n",
    "estimated_theta = steepest_ascent(10**2, theta_start, omega_start)\n",
    "estimated_theta.reshape(5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2, -3],\n",
       "       [ 2, -3],\n",
       "       [-2,  3],\n",
       "       [-2, -2],\n",
       "       [ 0,  2]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimates from steepest differ from the parameters as can be seen above. We should try different combinations of $\\epsilon$ and the starting point to get better results (we might be stuck at a local optimum). Similarly, we may need more data as $100$ samples might not be enough. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 {-}\n",
    "\n",
    "**Suppose that you know a certain stationary, mean zero Gaussian process $X(t)$ has autocorrelation given by $r(t) = \\sigma^2 \\exp[−\\lambda t]$ and that you know $\\sigma^2$ and $\\lambda$. An experiment provides a collection of samples $(t_i, x_i)$ for $i = 1, 2, . . . , N$. Describe how you would compute a p-value for the hypothesis that the $x_i$ are the values of $X(t)$ at the times $t_i$ from a single sample of the Gaussian process. No need to code anything up, just explain how you would compute a p-value and why your approach makes sense.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X \\in R^n$ be the stationary, mean zero Gaussian process specified above. Then $X \\sim N(0, \\Sigma)$ where $\\Sigma_{ij}=\\sigma^2\\exp[-\\lambda|i-j|]$.\n",
    "\n",
    "Let $\\Sigma = AA^T$ be the cholesky decomposition of $\\Sigma$. Then $Y=A^TX\\sim N(0,I)$ from previous HWs. I.e., we now have that $Y_i$ for $i=1,2,...,n$ are iid random variables each distributed as $N(0,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And from here we can apply the Kolmogorov Smirnoff statistical test. The null hypothesis assumes no difference between the observed ($Y_i$) and theoretical ($N(0,1)$) distribution and the value of the test statistic D is calculated as the supremum of the set of distances $D_n=\\sup_x |F_n(x)-F(x)|$. To get the p-value we can pass our $Y_i$ to a python or R Kolmogorov Smirnoff function (in Python scipy.stats.kstest returns the p-value).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
